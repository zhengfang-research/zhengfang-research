<!DOCTYPE html>
<html lang="zh-CN">
	<head>
		<title>The Blind Men and the Elephant: Introducing Machine-Learning-Based Data Fusion Methods for Multimodal Data</title>
		<meta name="description" content="">
		<meta name="author" content="">
		<meta name="keywords" content="">
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" href="css/uikit.min.css" />
		<link rel="stylesheet" href="css/main.css" />
		<script src="js/uikit.min.js"></script>
		<script src="js/jquery.min.js"></script>
		<script src="js/uikit-icons.min.js"></script>
        <script src="js/script.js"></script>
		</head>

	<body oncontextmenu=self.event.returnValue=false>

		<nav class="uk-navbar-container uk-margin" uk-navbar uk-sticky>
		    <div class="uk-navbar-left">
			        <a href="#offcanvas-slide" uk-icon="icon: menu" uk-toggle></a>

						<div id="offcanvas-slide" uk-offcanvas="mode: push">
						    <div class="uk-offcanvas-bar">

						        <ul class="uk-nav uk-nav-default">
						            <li class="uk-nav-header"><a href="#1" uk-scroll><b>1. Distribution of videos across product categories</b></a></li>
						            <li class="uk-nav-header"><a href="#2" uk-scroll><b>2. Live streaming commerce video sample</b></a></li>
						            <li class="uk-nav-header"><a href="#3" uk-scroll><b>3. Feature extraction summary</b></a></li>
						            <li><a href="#3.1" uk-scroll>3.1 Openface</a></li>
						            <li><a href="#3.2" uk-scroll>3.2 Covarep</a></li>
						            <li><a href="#3.3" uk-scroll>3.3 Alphapose</a></li>
						            <li class="uk-nav-header"><a href="#4" uk-scroll><b>4. Model Architecture</b></a></li>
						            <li><a href="#4.1" uk-scroll>4.1 Architecture of LSTM model with late fusion (LF-LSTM model)</a></li>
						            <li><a href="#4.2" uk-scroll>4.2 Architecture of LSTM model with early fusion (EF-LSTM model)</a></li>
						            <li class="uk-nav-header"><a href="#5" uk-scroll><b>5. Validation performance</b></a></li>
						            <li class="uk-nav-header"><a href="#6" uk-scroll><b>6. Results for multimodal trustworthiness analysis</b></a></li>
						        </ul>

						    </div>
						</div>
		    </div>

		        <div class="uk-navbar-right">
		        		<div class="uk-inline">
						    <a type="button" uk-icon="icon: world"></a>
						    <div uk-drop="mode: click">
						        <div class="uk-card uk-card-body uk-card-default" style="text-align: center;padding: 10px;width: auto;">
						        	<a href="https://zhengfang-research.github.io/multimodal_trustworthiness/" target="_blank" style="margin-right: 10px;">Gitub</a>
						        	<a href="https://zhengfang-research.gitee.io/multimodal_trustworthiness/" target="_blank">Gitee</a>
						        </div>
						    </div>
						</div>
			    </div>

		    
		</nav>


<div class="banner">
	<div class="uk-container uk-container-center container">
		<div class="large-title">
				The Blind Men and the Elephant: <br>Introducing Machine-Learning-Based Data Fusion Methods for Multimodal Data 
			</div>
			<div class="large-author" style="letter-spacing: 1px;">
				<p>Xueming Luo,  (<a href="https://www.fox.temple.edu/about-fox/directory/xueming-luo/" target="_blank">Temple University</a>)</p>
				<p>Nan Jia,  (<a href="https://www.marshall.usc.edu/personnel/nan-jia
" target="_blank">University of Southern California</a>)</p>
				<p>Zheng Fang,  (<a href="http://bs.scu.edu.cn/yingxiao/202104/7103.html" target="_blank">Sichuan University</a>)</p>
			</div>
			<div class="content" style="">
						<h1>Abstract</h1>
						<p>Multimodal data—unstructured text, image, and/or audio data that interdependently characterize the same source—offer a wealth of information for strategy researchers. We discuss the tradeoffs of using text versus non-text data and the conditions that call for incorporating non-text data to increase predictive accuracy. We then introduce cutting-edge machine deep learning and data fusion methods that holistically account for both intra- and inter-modality interactions, to automate the analyses of video data, the most prevalent form of multimodal data. We provide empirical demonstration by measuring the trustworthiness of sellers in live streaming commerce on TikTok and showing its importance to making sales. This paper expands the empirical toolbox for unstructured multimodal data analysis and advocates for data fusion in multiple areas of strategy research.</p>
						<p><a href="https://cmusatyalab.github.io/openface" class="uk-icon-button" uk-icon="github" target="_blank" style="background: #d9d9d9;"></a></p>
					</div>
	</div>
</div>

		<div class="uk-container uk-container-center container" id="content">
			<div class="catalog">
				<h1>Data, Models and Results of Machine Learning</h1>
				<h3>CONTENT</h3>
				<ul class="uk-nav uk-nav-default">
						            <li class="uk-nav-header"><a href="#1" uk-scroll><b>1. Distribution of videos across product categories</b></a></li>
						            <li class="uk-nav-header"><a href="#2" uk-scroll><b>2. Live streaming commerce video sample</b></a></li>
						            <li class="uk-nav-header"><a href="#3" uk-scroll><b>3. Feature extraction summary</b></a></li>
						            <li><a href="#3.1" uk-scroll>3.1 Openface</a></li>
						            <li><a href="#3.2" uk-scroll>3.2 Covarep</a></li>
						            <li><a href="#3.3" uk-scroll>3.3 Alphapose</a></li>
						            <li class="uk-nav-header"><a href="#4" uk-scroll><b>4. Model Architecture</b></a></li>
						            <li><a href="#4.1" uk-scroll>4.1 Architecture of LSTM model with late fusion (LF-LSTM model)</a></li>
						            <li><a href="#4.2" uk-scroll>4.2 Architecture of LSTM model with early fusion (EF-LSTM model)</a></li>
						            <li class="uk-nav-header"><a href="#5" uk-scroll><b>5. Validation performance</b></a></li>
						            <li class="uk-nav-header"><a href="#6" uk-scroll><b>6. Results for multimodal trustworthiness analysis</b></a></li>
						        </ul>			
			</div>

			<div class="uk-grid-small" uk-grid>
				<div class="uk-width-1-1">
				<div class="content">
						<h1 id="1">1. Distribution of observations across product categories</h1>
						
						<div class="uk-width-1-1" uk-lightbox="animation: slide" style="text-align: center;">
							<a href="media/1-1.webp" data-caption="" class="apic-pc"><img src="media/1-1.webp" alt="" style="width: 100%;"></a>
							<a href="media/1-2.webp" data-caption="" class="apic-h5"><img src="media/1-2.webp" alt="" style="width: 100%;"></a>
						</div>

				<h1 id="2">2. Live streaming commerce video sample</h1>
				<div class="video">
					<iframe height="" width="100%" src="https://player.bilibili.com/player.html?bvid=BV1vd4y1B74h&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
		        </div>
				</div>

				<h1 id="3">3. Feature extraction summary</h1>
					<div class="img-list" uk-lightbox="animation: slide">
						<div class="img-list-pic">
							<a href="media/1.webp" data-caption="Table 1. Summary of Multimodal Feature Extraction"><img src="media/1.webp" alt=""></a>
						</div>
					</div>

				<h2 id="3.1">3.1 Openface</h2>

				<p>
					The <span>OpenFace 2.0</span> conducts the following analysis. First, it uses the recently proposed <span>Convolutional Experts Constrained Local Model (CE-CLM)</span> (Amir et al., 2017) for facial landmark detection and tracking. Second, in order to estimate eye gaze, <span>OpenFace use a Constrained Local Neural Field (CLNF)</span> landmark detector  (Baltrusaitis et al., 2013; Wood et al., 2015) to detect eyelids, iris, and the pupil. Third, <span>OpenFace 2.0</span> recognizes facial expressions through detecting facial action unit (AU) intensity and presence. It uses a method based on a recent AU recognition framework by Baltruˇsaitis et al. (Baltrušaitis et al., 2015), that uses linear kernel <span>Support Vector Machines (SVM)</span>. Fourth, it is able to extract head pose as CE-CLM (Consrained Local Model) internally uses a 3D representation of facial landmarks and projects them to the image using orthographic camera projection (Baltrusaitis et al., 2018). For improved efficiency, we extract the first image of each second to calculate the features of the visual modality (Yang et al., 2021; Zhang et al., 2021). Therefore, a 135 s × 49 d matrix is extracted to represent the sellers’ facial features for a typical video in our dataset. Figure A4.1 uses an example to show the facial landmarks and list the facial features extracted.
				</p>
				<div class="uk-grid-small" uk-grid uk-height-match>
				<div class="uk-width-1-1 uk-width-1-3@m">
					<div class="video2">
						<iframe height="" width="100%" src="https://player.bilibili.com/player.html?bvid=BV1pT411u7bq&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
					</div>
				</div>
				<div class="uk-width-1-1 uk-width-2-3@m" uk-lightbox="animation: slide">
					<a href="media/2.webp" data-caption=""><img src="media/2.webp" alt=""></a>
				</div>
				</div>

				<h2 id="3.2">3.2 Covarep</h2>
				<div class="uk-grid-small" uk-grid>
					<div class="uk-width-1-1">
						<p>The interconnection between algorithms in Covarep include five aspects. Information necessary to perform pitch-synchronous analysis is first extracted from the speech signal: pitch tracking (Summation of the Residual Harmonics [SRH] method), polarity detection and determination of the Glottal Closure Instants (GCIs). The resulting information is generally required to guarantee the high performance of subsequent methods: spectral envelope estimation (called “true-envelope”[TE] computed directly on the spectrum of the discrete Fourier transform (DFT) of a windowed speech signal.) (Imai, 1983; Imai & Abe, 1979; Röbel et al., 2007) and formant tracking (This algorithm is based on processing the negative derivative of the argument of the chirp-z transform [termed as the differential phase, or group-delay spectrum]), sinusoidal modeling (including Harmonic Model [HM], the quasiharmonic model, the adaptive quasi-harmonic model, the Adaptive Harmonic Model [aHM], the extended adaptive quasi-harmonic model) (Degottex & Stylianou, 2013; Kafentzis et al., 2012; Pantazis et al., 2008, 2010; Stylianou, 1996), glottal analysis (including Glottal flow (GF) estimation and GF parameters) and phase processing (using Relative Phase Shift [RPS] and Phase Distortion [PD] measurement)</p>
						<p>The figure above in Figure A3.1 is the waveform of the signal.  A signal is the change in a certain amount over time. For audio, the change is air pressure. We take barometric samples over time at a sampling rate of 44.1kHz, which means there are 44,100 samples per second. What we captured is the waveform of the signal, shown in the following figure.  It can be draw leveraging librosa. </p>
						<p>Though Fourier transform, we could obtain the spectrogram for analyzing the frequency component of the signal. It is a method for representing the spectrum of these signals over time and it is a way to visually represent the loudness or amplitude of a signal as it varies over time at different frequencies. After four steps of calculating the absolute value, taking the logarithm, phase unwrapping and inverse Fourier transform, we obtain the cepstrum. Because humans cannot perceive frequencies in a linear range. We are better at detecting differences in low frequencies than high frequencies. In order to make it more similar to the human auditory system, we divided the frequency bands of cepstrum into equidistant divisions according to the MEL scale to obtain the Mel Frequency Cepstrum Coefficient (MFCC). It can be obtained and visualized though librosa.</p>
					</div>
					<div class="uk-width-1-1" uk-lightbox="animation: slide">
					<div style="max-width: 650px;margin:auto">
						<a href="media/3.webp" data-caption=""><img src="media/3.webp" alt=""></a>
					</div>
				</div>
				</div>

				<h2 id="3.3">3.3 Alphapose</h2>
				<div class="uk-grid-medium" uk-grid>
				<div class="uk-width-1-1">
					<p>Body features are extracted based on <span>AlphaPose</span> (Fang et al., 2017) the state-of-the-art multi-person body pose estimation system. It consists of three components: <span>Symmetric Spatial Transformer Network (SSTN)</span>, Parametric Pose Non-Maximum-Suppression (NMS), and <span>Pose-Guided Proposals Generator (PGPG)</span>. Among them, <span>SSTN</span> is designed to extract a high-quality single person region from an inaccurate bounding box. Its parametric pose <span>NMS</span> eliminates redundant poses by using a novel pose distance metric to compare pose similarity. And <span>PGPG</span>, a novel pose-guided human proposal generator is used to augment a large number of training samples. Using the existing model based on AlphaPose which detects 17 body key points, we can directly obtain the visualized connection of body movement.</p>
				</div>
				<div class="video">
					<iframe height="" width="100%" src="https://player.bilibili.com/player.html?bvid=BV1m24y1d7pL&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
				</div>
				</div>

				<h1 id="4">4. Model Architecture</h1>
				<h3 id="4.1">4.1 Architecture of LSTM model with late fusion (LF-LSTM model)</h3>
				<div class="img-list" uk-lightbox="animation: slide">
						<div class="img-list-pic">
							<a href="media/4.webp" data-caption="TFigure 4.1 Architecture of LSTM model with late fusion (LF-LSTM model)"><img src="media/4.webp" alt=""></a>
						</div>
					</div>
				<h3 style="margin-top: 80px;" id="4.2">4.2 Architecture of LSTM model with early fusion (EF-LSTM model)</h3>
				<div class="img-list" uk-lightbox="animation: slide">
						<div class="img-list-pic">
							<a href="media/5.webp" data-caption="Figure 4.2 Architecture of LSTM model with early fusion (EF-LSTM model)"><img src="media/5.webp" alt=""></a>
						</div>
					</div>

				<h1 id="5">5. Validation performance</h1>
				<div class="tab-h5">
					<div class='tableauPlaceholder' id='viz1663433828808' style='position: relative'><noscript><a href='#'><img alt='故事 1 ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;ta&#47;tabledata_16630601828690&#47;1_1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='tabledata_16630601828690&#47;1_1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;ta&#47;tabledata_16630601828690&#47;1_1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='language' value='zh-CN' /><param name='filter' value='publish=yes' /></object><div class="x"></div></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1663433828808');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.minWidth='100%';vizElement.style.maxWidth='100%';vizElement.style.width='100%';vizElement.style.minHeight='427px';vizElement.style.maxHeight='477px';vizElement.style.height='430px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>
				</div>
				<div class="tab-pc">
					<ul class="uk-subnav uk-subnav-pill" uk-switcher>
					    <li><a href="#">Figure 5.1 Acc2</a></li>
					    <li><a href="#">Figure 5.2 F1-Sccore</a></li>
					    <li><a href="#">Figure 5.3 MAE</a></li>
					    <li><a href="#">Figure 5.4 MSE</a></li>
					    <li><a href="#">Figure 5.5 Corr</a></li>
					</ul>
				

				<ul class="uk-switcher loading">
				    <li>
				    	<div class="tableau">
				    		<div class='tableauPlaceholder' id='viz1663422618267' style='position: relative'><noscript><a href='#'><img alt='Figure 5.1 Acc2 ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;ta&#47;tabledata_16630601828690&#47;1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='tabledata_16630601828690&#47;1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;ta&#47;tabledata_16630601828690&#47;1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='language' value='zh-CN' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1663422618267');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height='400px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>
				    		<div class="x">Epochs</div>
				    	</div>
				    	<div class="tips">Note:  “CTC+EF-LSTM” refers to the same model as the abbreviated term “EF-LSTM.”</div>
				    </li>
				    <li>
				    	<div class="tableau">
				    		<div class='tableauPlaceholder' id='viz1663428061166' style='position: relative'><noscript><a href='#'><img alt='工作表 2 ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;ta&#47;tabledata_16630601828690&#47;2&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='tabledata_16630601828690&#47;2' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;ta&#47;tabledata_16630601828690&#47;2&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='language' value='zh-CN' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1663428061166');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height='400px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>
				    		<div class="x">Epochs</div>
				    	</div>
				    </li>
				    <li>
				    	<div class="tableau">
				    		<div class='tableauPlaceholder' id='viz1663428175422' style='position: relative'><noscript><a href='#'><img alt='工作表 3 ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;ta&#47;tabledata_16630601828690&#47;3&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='tabledata_16630601828690&#47;3' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;ta&#47;tabledata_16630601828690&#47;3&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='language' value='zh-CN' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1663428175422');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height='400px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>
				    		<div class="x">Epochs</div>
				    	</div>
				    </li>
				    <li>
				    	<div class="tableau">
				    		<div class='tableauPlaceholder' id='viz1663428197905' style='position: relative'><noscript><a href='#'><img alt='工作表 4 ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;ta&#47;tabledata_16630601828690&#47;4&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='tabledata_16630601828690&#47;4' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;ta&#47;tabledata_16630601828690&#47;4&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='language' value='zh-CN' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1663428197905');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height='400px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>
				    		<div class="x">Epochs</div>
				    	</div>
				    </li>
				    <li>
				    	<div class="tableau">
				    		<div class='tableauPlaceholder' id='viz1663428295327' style='position: relative'><noscript><a href='#'><img alt='工作表 5 ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;ta&#47;tabledata_16630601828690&#47;5&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='tabledata_16630601828690&#47;5' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;ta&#47;tabledata_16630601828690&#47;5&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='language' value='zh-CN' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1663428295327');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height='400px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>
				    		<div class="x">Epochs</div>
				    	</div>
				    </li>
				</ul>

				</div>



				<div class="color">
					<ul>
						<li><div style="background: #db2a43" class="color-model"></div>CTC+EF-LSTM (verbal+vocal+visual) </li>
						<li><div style="background: #92d587" class="color-model"></div>LF-LSTM (verbal+vocal+visual) </li>
						<li><div style="background: #ff7252" class="color-model"></div>CTC+EF-LSTM (verbal+vocal) </li>
						<li><div style="background: #ffc5ba" class="color-model"></div>CTC+EF-LSTM (verbal+visual) </li>
						<li><div style="background: #f8a842" class="color-model"></div>CTC+EF-LSTM (vocal+visual) </li>
						<li><div style="background: #535c69" class="color-model"></div>LF-LSTM (verbal+vocal) </li>						
						<li><div style="background: #ffe06b" class="color-model"></div>LF-LSTM (verbal+visual)</li>
						<li><div style="background: #2e9d49" class="color-model"></div>LF-LSTM (vocal+visual) </li>
						<li><div style="background: #a9d1ec" class="color-model"></div>LSTM (verbal) </li>
						<li><div style="background: #c2b9b5" class="color-model"></div>LSTM (vocal) </li>					
						<li><div style="background: #6d9ec7" class="color-model"></div>LSTM (visual)</li>				
					</ul>
				</div>

				</div>

			<h1 id="6">6. Results for multimodal trustworthiness analysis</h1>
			<div class="img-list" uk-lightbox="animation: slide">
						<div class="img-list-pic">
							<a href="media/6.png" data-caption="Table *. Performance of Binary-Class and Multi-Class Classifications by Different Deep Learning Models"><img src="media/6.png" alt=""></a>
						</div>
					</div>
			<div class="img-model6">
					<img src="media/7.png" alt="">
			</div>
			<div class="img-model7">
					<img src="media/8.png" alt="">
			</div>

				</div>
			</div>
		</div>


<a href="" uk-totop class="totop" uk-scroll><span uk-icon="top"></span></a>
	</body>
</html>